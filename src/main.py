import argparse
import json
import random
import sys
import os
from datetime import datetime
from pathlib import Path
from dataclasses import asdict
from typing import Dict, Any, Tuple
import matplotlib.font_manager as fm

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import StandardScaler
from xgboost import XGBRegressor
import joblib
import optuna
import psycopg2
import psycopg2.extras

from src.data.load_data import load_and_prepare
from src.data.preprocess import preprocess_load_metric
from src.features.feature_engineering import make_features
# train_tune.pyì— ìˆëŠ” í•¨ìˆ˜ë“¤ì„ ì§ì ‘ ê°€ì ¸ì˜¤ëŠ” ê²ƒìœ¼ë¡œ ê°€ì •í•©ë‹ˆë‹¤.
from src.modeling.train_tune import set_seed, build_dataset, train_evaluate, FeatureConfig, ModelConfig, objective, \
    time_split
from src.modeling.validate import build_dataset_for_validation, save_validation_results_to_db, load_db_config


def main():
    BASE_DIR = Path(__file__).resolve().parent.parent
    DATA_PATH = BASE_DIR / "src" / "data" / "node_metrics.csv"
    MODELS_DIR = BASE_DIR / "src" / "models"
    MODELS_DIR.mkdir(parents=True, exist_ok=True)

    parser = argparse.ArgumentParser()
    parser.add_argument("--csv", default=DATA_PATH, help="ì›ë³¸ CSV ê²½ë¡œ")
    parser.add_argument("--valid_ratio", type=float, default=0.2, help="ê²€ì¦ ë¹„ìœ¨")
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--trials", type=int, default=100, help="ìµœì í™” ì‹œë„ íšŸìˆ˜")
    parser.add_argument("--out_prefix", default="best_model", help="ì‚°ì¶œë¬¼ íŒŒì¼ëª… ì ‘ë‘ì‚¬")
    parser.add_argument("--db-config", default=BASE_DIR / "src" / "config" / "db_config.json", help="DB ì ‘ì† ì •ë³´ JSON ê²½ë¡œ")
    args = parser.parse_args()

    set_seed(args.seed)

    # =========================================================
    # Step 1: ë°ì´í„° ë¡œë“œ, ì „ì²˜ë¦¬ ë° ë¶„í• 
    # =========================================================
    print("----- Step 1: ë°ì´í„° ë¡œë“œ, ì „ì²˜ë¦¬ ë° ë¶„í•  -----")
    df_part1_path = BASE_DIR / "src" / "data" / f"{args.out_prefix}_preprocessed_part1.csv"
    df_part2_path = BASE_DIR / "src" / "data" / f"{args.out_prefix}_preprocessed_part2.csv"

    if df_part1_path.exists() and df_part2_path.exists():
        print(f"âœ… ì „ì²˜ë¦¬ëœ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. {df_part1_path}ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
        df_processed = pd.read_csv(df_part1_path, index_col='timestamp', parse_dates=True)
    else:
        print("ğŸ“ì „ì²˜ë¦¬ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        df_raw = load_and_prepare(DATA_PATH)
        print("ì›ë³¸ ë°ì´í„° í¬ê¸°:", df_raw.shape)

        df_processed_all = preprocess_load_metric(df_raw)
        print("ì „ì²˜ë¦¬ëœ ë°ì´í„° í¬ê¸°:", df_processed_all.shape)

        mid_point = len(df_processed_all) // 2
        df_part1 = df_processed_all.iloc[:mid_point].copy()
        df_part2 = df_processed_all.iloc[mid_point:].copy()

        df_part1.to_csv(df_part1_path)
        df_part2.to_csv(df_part2_path)
        print(f"âœ… ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {df_part1_path} ì™€ {df_part2_path}")

        df_processed = df_part1

    # =========================================================
    # Step 2: ë² ì´ì§€ì•ˆ ìµœì í™”ë¥¼ í†µí•œ ëª¨ë¸ íŠœë‹
    # =========================================================
    print("\n----- Step 2: ë² ì´ì§€ì•ˆ ìµœì í™” ì‹œì‘ -----")
    study = optuna.create_study(direction='minimize')
    print(f"ğŸ” ì´ {args.trials}íšŒ íƒìƒ‰.")
    # objective í•¨ìˆ˜ì— valid_ratio ì „ë‹¬
    objective_func = lambda trial: objective(trial, df_processed, valid_ratio=args.valid_ratio)
    study.optimize(objective_func, n_trials=args.trials, show_progress_bar=True)
    best_trial = study.best_trial

    # ìµœì  MAE ê°’ì„ ë³€ìˆ˜ì— ì €ì¥
    best_mae = best_trial.value
    print(f"\nâœ¨ ìµœì í™” ì™„ë£Œ! Best MAE: {best_mae:.4f}")

    # ëª¨ë“  íƒìƒ‰ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
    print("\nâœ… ëª¨ë“  íƒìƒ‰ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥ ì¤‘...")
    try:
        df_trials = study.trials_dataframe()
        df_trials.to_csv(MODELS_DIR / f"{args.out_prefix}_trials.csv", index=False)
        print(f"âœ… ëª¨ë“  íƒìƒ‰ ê²°ê³¼ê°€ {MODELS_DIR / f'{args.out_prefix}_trials.csv'} íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ íƒìƒ‰ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    # =========================================================
    # Step 3: ìµœì  ëª¨ë¸ ì¬í•™ìŠµ ë° ì €ì¥
    # =========================================================
    print("\n----- Step 3: ìµœì  êµ¬ì„±ìœ¼ë¡œ ëª¨ë¸ ì¬í•™ìŠµ ë° ì €ì¥ -----")
    feat_cfg = FeatureConfig(**{k: v for k, v in best_trial.params.items() if k in FeatureConfig.__annotations__})
    model_cfg = ModelConfig(**{k: v for k, v in best_trial.params.items() if k in ModelConfig.__annotations__})

    # <<< ìˆ˜ì •: ìµœì¢… ëª¨ë¸ì€ df_part1 ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ (ë°ì´í„° ë¶„í•  X)
    X_train, y_train = build_dataset(df_processed, feat_cfg)

    float_cols = X_train.select_dtypes(include=["float32", "float64"]).columns
    scaler = StandardScaler()
    X_train.loc[:, float_cols] = scaler.fit_transform(X_train[float_cols])

    model = XGBRegressor(
        n_estimators=model_cfg.n_estimators, max_depth=model_cfg.max_depth, learning_rate=model_cfg.learning_rate,
        subsample=model_cfg.subsample, colsample_bytree=model_cfg.colsample_bytree, reg_alpha=model_cfg.reg_alpha,
        reg_lambda=model_cfg.reg_lambda, min_child_weight=model_cfg.min_child_weight, random_state=42, n_jobs=-1,
        tree_method="hist")
    model.fit(X_train, y_train, verbose=False)

    # <<< ìˆ˜ì •: MAEë¥¼ ìƒˆë¡œ ê³„ì‚°í•˜ì§€ ì•Šê³ , Optunaê°€ ì°¾ì€ ìµœì ì˜ ê°’ì„ ì‚¬ìš©
    meta = {
        "trial": {
            'feat_cfg': asdict(feat_cfg),
            'model_cfg': asdict(model_cfg),
            'mae': best_mae  # Optunaì˜ ê²°ê³¼ê°’ ì‚¬ìš©
        },
        "final_valid_mae": best_mae,  # Optunaì˜ ê²°ê³¼ê°’ ì‚¬ìš©
        "feature_names": list(X_train.columns),
        "valid_ratio": args.valid_ratio,  # íŠœë‹ ì‹œ ì‚¬ìš©ëœ ë¹„ìœ¨ ê¸°ë¡
        "csv_path": str(args.csv),
        "training_data_shape": X_train.shape,
    }
    joblib.dump(model, MODELS_DIR / f"{args.out_prefix}_xgb_model.joblib")
    joblib.dump(scaler, MODELS_DIR / f"{args.out_prefix}_scaler.joblib")
    with open(MODELS_DIR / f"{args.out_prefix}_config.json", "w") as f:
        json.dump(meta, f, indent=2, ensure_ascii=False)
    print("âœ… ëª¨ë¸ ë° ì„¤ì • íŒŒì¼ ì €ì¥ ì™„ë£Œ")
    print(f"  - ìµœì¢… ê²€ì¦ MAE (from Optuna): {best_mae:.4f}")

    # =========================================================
    # Step 4: df_part2 ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ ê²€ì¦
    # =========================================================
    print("\n----- Step 4: df_part2 ë°ì´í„°ë¡œ ëª¨ë¸ ê²€ì¦ -----")
    X_val, y_val = build_dataset_for_validation(df_part2_path, feat_cfg)

    # ëˆ„ë½ëœ ì»¬ëŸ¼ì´ ìˆì„ ê²½ìš° 0ìœ¼ë¡œ ì±„ìš°ê¸° (í›ˆë ¨ ì‹œì ì—ëŠ” ìˆì—ˆìœ¼ë‚˜ ê²€ì¦ ì‹œì ì—ëŠ” ì—†ëŠ” í”¼ì²˜)
    missing_cols = set(X_train.columns) - set(X_val.columns)
    for c in missing_cols:
        X_val[c] = 0
    X_val = X_val[X_train.columns]  # í›ˆë ¨ ë°ì´í„°ì™€ ì»¬ëŸ¼ ìˆœì„œ ì¼ì¹˜

    if float_cols is not None:
        cols_to_scale = [c for c in float_cols if c in X_val.columns]
        if cols_to_scale:
            X_val.loc[:, cols_to_scale] = scaler.transform(X_val[cols_to_scale])

    y_pred_np = model.predict(X_val)
    y_pred = np.array([float(x) for x in y_pred_np])
    final_mae = mean_absolute_error(y_val, y_pred)
    horizon = feat_cfg.horizon

    print("\nâœ… df_part2 ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ì™„ë£Œ")
    print(f"  - ì˜ˆì¸¡ ì‹œì : t+{horizon} ({int(horizon) * 5}ë¶„ í›„)")
    print(f"  - ìµœì¢… ê²€ì¦ MAE (on df_part2): {final_mae:.4f}")

    # DB ì €ì¥
    db_config = load_db_config(args.db_config)
    save_validation_results_to_db(final_mae, y_val, y_pred, int(horizon), db_config)

    # =========================================================
    # <<< Step 5: ì‹œê°í™” ê²°ê³¼ ì €ì¥ (ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„)
    # =========================================================
    font_path = None
    if os.name == 'posix':
        try:
            if 'Darwin' in os.uname():
                font_path = '/System/Library/Fonts/AppleGothic.ttf'
            elif os.path.exists('/usr/share/fonts/truetype/nanum/NanumGothic.ttf'):
                font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'
        except:
            pass
    elif os.name == 'nt':
        font_path = 'C:/Windows/Fonts/malgunbd.ttf'

    if font_path and os.path.exists(font_path):
        font_name = fm.FontProperties(fname=font_path).get_name()
        plt.rc('font', family=font_name)
    else:
        for font in fm.fontManager.ttflist:
            if 'Gothic' in font.name or 'Malgun' in font.name or 'Nanum' in font.name:
                plt.rc('font', family=font.name)
                break
    plt.rcParams['axes.unicode_minus'] = False
    # ===== 1. plot ì‹œê°í™” =====
    print("\nğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”...")

    plt.figure(figsize=(15, 7))
    plt.plot(y_val.index, y_val, label='ì‹¤ì œ ê°’ (df_part2)', color='blue', alpha=0.7)
    plt.plot(y_val.index, y_pred, label='ì˜ˆì¸¡ ê°’', color='red', linestyle='--')

    plt.title(f"df_part2 ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ (t+{horizon} = {int(horizon) * 5}ë¶„ í›„)")
    plt.xlabel("ì‹œì ")
    plt.ylabel("Node Load1")
    plt.legend()
    plt.grid(True)

    plt.xticks(rotation=45)
    plt.tight_layout()

    # ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥í•˜ê¸°
    save_dir = Path.home() / "Code" / "ntels_Project" / "result"
    save_dir.mkdir(parents=True, exist_ok=True)
    ts_str = datetime.now().strftime("%Y%m%d-%H%M%S")
    outfile = save_dir / f"{ts_str}_ë² ì´ì§€ì•ˆìµœì í™”.png"

    fig = plt.gcf()  # í˜„ì¬ figure ê°ì²´
    fig.savefig(outfile, dpi=150, bbox_inches="tight")
    print(f"plot ì €ì¥ ì™„ë£Œ : {outfile}")

    plt.show()


if __name__ == "__main__":
    main()